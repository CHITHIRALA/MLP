---
title: "Model using Random FOrest and prdict weight lifting performance"
author: "RC"
date: "Friday, November 21, 2014"
output: html_document
---
###Abstract
This illustrates use of a random forest method of prediction for classifying the performance of individual dumbbell bicep curls. A large weight lifting data set, with measurements taken from several accelerometers, was first cleaned and then partitioned into a training and a test set. After obtaining a model, cross-validation revealed that is highly accurate(.996) with sample error rate of .004. I applied the same model later on testing data set of 20 observations and my submission is correct for all twenty questions.

Ignore the warnings
```{r}
  options(warn=-1);
```

Load the required packages
```{r }
  library(caret);library(randomForest);library(Hmisc);library(foreach);library(doParallel) 
```
 
To be reproducible, set the seed
```{r}
 set.seed(4356)
```

Read training data
```{r}
 trainingFile = "pml-training.csv"
 if (!file.exists(trainingFile)) {

     download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = trainingFile, method ="curl")
     
 }

 data <- read.csv(trainingFile)

```

View the data characterstics. Due to space constraints, I commented the commands.
```{r}
 #summary(data)
 #describe(data)
 #sapply(data, class) 
 #str(data)
```

By looking at the results from the above commands, it is clear that we have two issues. 1: There are values #DIV/0! and some columns(variables) have very low completion rate (NA values)

Reload the data by ignoring #Div/0! 
```{r}
 datan <- read.csv(trainingFile, na.strings=c("#DIV/0!") )
  
```

Convert the factor values to numeric values excluding the predictor and first 7 columns
```{r}
 cData <- datan
for(i in c(8:ncol(cData)-1)) {cData[,i] = as.numeric(as.character(cData[,i]))}
```

Get the columns with complete data. We also filter first seven columns, which are time stamps, number, name not useful for prediction 
```{r}
 featuresnames <- colnames(cData[colSums(is.na(cData)) == 0])[-(1:7)]
 features <- cData[featuresnames]
```

We now have a dataframe "features"" which contains all the workable features. So the first step is to split the dataset in two part : the first for training(75%) and the second for testing(25%).
```{r}
xdata <- createDataPartition(y=features$classe, p=3/4, list=FALSE )
training <- features[xdata,]
testing <- features[-xdata,]
```

Use the randomForest to train and model. A forest with 150 trees
```{r}
 model <- randomForest(training[-ncol(training)], training$classe, ntree=150)
```

To evaluate the model we will use the confusionmatrix method and we will focus on accuracy, sensitivity & specificity metrics. This also shows cross vadation of predicted results with the real values. 

Now apply the model to the testing data
```{r}
predictionsTe <- predict(model, newdata=testing)
confusionMatrix(predictionsTe,testing$classe)
```
As seen by the result of the confusionmatrix, the model is good and efficient because it has an accuracy of 0.996 and very good sensitivity & specificity values on the testing dataset. (the lowest value is 0.989 for the sensitivity of the class C). The expected sample error rate is 0.004.



Below is the code I used to the model above for predicting outcome for testing data set. I submitted the predictions and all 20 are correct. This also proves that this is a good model.

```{r}
testingFile = "pml-testing.csv"
 if (!file.exists(testingFile)) {

     download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = testingFile, method ="curl")
     
 }

 testingd <- read.csv(testingFile,na.strings=c("#DIV/0!"))


 cDatat <- testingd
for(i in c(8:ncol(cData)-1)) {cData[,i] = as.numeric(as.character(cData[,i]))}

 featuresnames <- colnames(cDatat[colSums(is.na(cDatat)) == 0])[-(1:7)]
 features <- cDatat[featuresnames]
 predictionsTesting <- predict(model,newdata=features)

```

 
 